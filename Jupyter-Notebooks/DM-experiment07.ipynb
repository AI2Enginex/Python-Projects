{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DM-experiment07.ipynb","provenance":[],"mount_file_id":"1nB4zLVWvYR5bWde2kFuqX8rELHrFd7Cl","authorship_tag":"ABX9TyPQ/nxMaJgGELxoryyf/nmh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import collections\n","import gym\n","import numpy as np\n","import statistics\n","import tensorflow as tf\n","import tqdm\n","\n","from matplotlib import pyplot as plt\n","from tensorflow.keras import layers\n","from typing import Any, List, Sequence, Tuple\n","\n","\n","# Create the environment\n","env = gym.make(\"CartPole-v0\")\n","\n","# Set seed for experiment reproducibility\n","seed = 42\n","env.seed(seed)\n","tf.random.set_seed(seed)\n","np.random.seed(seed)\n","\n","# Small epsilon value for stabilizing division operations\n","eps = np.finfo(np.float32).eps.item()"],"metadata":{"id":"vaW8ZbYGtf-t","executionInfo":{"status":"ok","timestamp":1653448847685,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class ActorCritic(tf.keras.Model):\n","  \"\"\"Combined actor-critic network.\"\"\"\n","\n","  def __init__(\n","      self, \n","      num_actions: int, \n","      num_hidden_units: int):\n","    \"\"\"Initialize.\"\"\"\n","    super().__init__()\n","\n","    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n","    self.actor = layers.Dense(num_actions)\n","    self.critic = layers.Dense(1)\n","\n","  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n","    x = self.common(inputs)\n","    return self.actor(x), self.critic(x)\n","\n","num_actions = env.action_space.n  # 2\n","num_hidden_units = 128\n","\n","model = ActorCritic(num_actions, num_hidden_units)"],"metadata":{"id":"070pm40yKrbT","executionInfo":{"status":"ok","timestamp":1653448933803,"user_tz":-330,"elapsed":429,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n","# This would allow it to be included in a callable TensorFlow graph.\n","\n","def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n","\n","  state, reward, done, _ = env.step(action)\n","  return (state.astype(np.float32), \n","          np.array(reward, np.int32), \n","          np.array(done, np.int32))\n","\n","\n","def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n","  return tf.numpy_function(env_step, [action], \n","                           [tf.float32, tf.int32, tf.int32])"],"metadata":{"id":"ZzNc_PXRK8wl","executionInfo":{"status":"ok","timestamp":1653448949619,"user_tz":-330,"elapsed":451,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def run_episode(\n","    initial_state: tf.Tensor,  \n","    model: tf.keras.Model, \n","    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n","  \"\"\"Runs a single episode to collect training data.\"\"\"\n","\n","  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n","  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n","  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n","\n","  initial_state_shape = initial_state.shape\n","  state = initial_state\n","\n","  for t in tf.range(max_steps):\n","    # Convert state into a batched tensor (batch size = 1)\n","    state = tf.expand_dims(state, 0)\n","  \n","    # Run the model and to get action probabilities and critic value\n","    action_logits_t, value = model(state)\n","  \n","    # Sample next action from the action probability distribution\n","    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n","    action_probs_t = tf.nn.softmax(action_logits_t)\n","\n","    # Store critic values\n","    values = values.write(t, tf.squeeze(value))\n","\n","    # Store log probability of the action chosen\n","    action_probs = action_probs.write(t, action_probs_t[0, action])\n","  \n","    # Apply action to the environment to get next state and reward\n","    state, reward, done = tf_env_step(action)\n","    state.set_shape(initial_state_shape)\n","  \n","    # Store reward\n","    rewards = rewards.write(t, reward)\n","\n","    if tf.cast(done, tf.bool):\n","      break\n","\n","  action_probs = action_probs.stack()\n","  values = values.stack()\n","  rewards = rewards.stack()\n","  \n","  return action_probs, values, rewards"],"metadata":{"id":"5eWSYJkJPLlR","executionInfo":{"status":"ok","timestamp":1653448967944,"user_tz":-330,"elapsed":477,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def get_expected_return(\n","    rewards: tf.Tensor, \n","    gamma: float, \n","    standardize: bool = True) -> tf.Tensor:\n","  \"\"\"Compute expected returns per timestep.\"\"\"\n","\n","  n = tf.shape(rewards)[0]\n","  returns = tf.TensorArray(dtype=tf.float32, size=n)\n","\n","  # Start from the end of `rewards` and accumulate reward sums\n","  # into the `returns` array\n","  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n","  discounted_sum = tf.constant(0.0)\n","  discounted_sum_shape = discounted_sum.shape\n","  for i in tf.range(n):\n","    reward = rewards[i]\n","    discounted_sum = reward + gamma * discounted_sum\n","    discounted_sum.set_shape(discounted_sum_shape)\n","    returns = returns.write(i, discounted_sum)\n","  returns = returns.stack()[::-1]\n","\n","  if standardize:\n","    returns = ((returns - tf.math.reduce_mean(returns)) / \n","               (tf.math.reduce_std(returns) + eps))\n","\n","  return returns"],"metadata":{"id":"4eA_6aa7PQCi","executionInfo":{"status":"ok","timestamp":1653448982788,"user_tz":-330,"elapsed":465,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n","\n","def compute_loss(\n","    action_probs: tf.Tensor,  \n","    values: tf.Tensor,  \n","    returns: tf.Tensor) -> tf.Tensor:\n","  \"\"\"Computes the combined actor-critic loss.\"\"\"\n","\n","  advantage = returns - values\n","\n","  action_log_probs = tf.math.log(action_probs)\n","  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n","\n","  critic_loss = huber_loss(values, returns)\n","\n","  return actor_loss + critic_loss"],"metadata":{"id":"euP_0rZ7PTpR","executionInfo":{"status":"ok","timestamp":1653449069100,"user_tz":-330,"elapsed":498,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n","\n","\n","@tf.function\n","def train_step(\n","    initial_state: tf.Tensor, \n","    model: tf.keras.Model, \n","    optimizer: tf.keras.optimizers.Optimizer, \n","    gamma: float, \n","    max_steps_per_episode: int) -> tf.Tensor:\n","  \"\"\"Runs a model training step.\"\"\"\n","\n","  with tf.GradientTape() as tape:\n","\n","    # Run the model for one episode to collect training data\n","    action_probs, values, rewards = run_episode(\n","        initial_state, model, max_steps_per_episode) \n","\n","    # Calculate expected returns\n","    returns = get_expected_return(rewards, gamma)\n","\n","    # Convert training data to appropriate TF tensor shapes\n","    action_probs, values, returns = [\n","        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n","\n","    # Calculating loss values to update our network\n","    loss = compute_loss(action_probs, values, returns)\n","\n","  # Compute the gradients from the loss\n","  grads = tape.gradient(loss, model.trainable_variables)\n","\n","  # Apply the gradients to the model's parameters\n","  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","  episode_reward = tf.math.reduce_sum(rewards)\n","\n","  return episode_reward"],"metadata":{"id":"DcB1I7cCPovG","executionInfo":{"status":"ok","timestamp":1653449083248,"user_tz":-330,"elapsed":450,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","min_episodes_criterion = 100\n","max_episodes = 10000\n","max_steps_per_episode = 1000\n","\n","# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n","# consecutive trials\n","reward_threshold = 195\n","running_reward = 0\n","\n","# Discount factor for future rewards\n","gamma = 0.99\n","\n","# Keep last episodes reward\n","episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n","\n","with tqdm.trange(max_episodes) as t:\n","  for i in t:\n","    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n","    episode_reward = int(train_step(\n","        initial_state, model, optimizer, gamma, max_steps_per_episode))\n","    \n","    episodes_reward.append(episode_reward)\n","    running_reward = statistics.mean(episodes_reward)\n","  \n","    t.set_description(f'Episode {i}')\n","    t.set_postfix(\n","        episode_reward=episode_reward, running_reward=running_reward)\n","  \n","    # Show average episode reward every 10 episodes\n","    if i % 10 == 0:\n","      pass # print(f'Episode {i}: average reward: {avg_reward}')\n","  \n","    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n","        break\n","\n","print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"twVJ8gF1PsMA","executionInfo":{"status":"ok","timestamp":1653449133572,"user_tz":-330,"elapsed":36680,"user":{"displayName":"Vibhor Sakalley","userId":"08508309510612408116"}},"outputId":"053b2f0b-aa65-40f7-8fd8-974c4710d01e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Episode 627:   6%|â–‹         | 627/10000 [00:36<09:01, 17.32it/s, episode_reward=200, running_reward=195]"]},{"output_type":"stream","name":"stdout","text":["\n","Solved at episode 627: average reward: 195.09!\n","CPU times: user 43.2 s, sys: 1.8 s, total: 45 s\n","Wall time: 36.2 s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"b4cyuqR3QBI8"},"execution_count":null,"outputs":[]}]}